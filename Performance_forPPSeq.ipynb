{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "Fs = 30000.0\n",
    "\n",
    "def parts(list_, indices):\n",
    "    indices = [0]+indices+[len(list_)]\n",
    "    return [list_[v:indices[k+1]] for k, v in enumerate(indices[:-1])]\n",
    "\n",
    "def RemoveSlowSequences(split,split2):\n",
    "    timefiltered_split = []\n",
    "    for i,item in enumerate(split2):\n",
    "        if item[0] == 1:\n",
    "            timefiltered_split = timefiltered_split + [split[i]]\n",
    "\n",
    "    return(timefiltered_split)\n",
    "\n",
    "def aligntofirstpokeandremovesingletransits(timesplitseqs,timesplitlatencies):\n",
    "    \n",
    "    newseqs = []\n",
    "    newlatencies = []\n",
    "    # align to first poke:\n",
    "    for index_1,fragments in enumerate(timesplitseqs):\n",
    "        current_newseqs = []\n",
    "        current_newlatencies = []\n",
    "        count = -1\n",
    "        seqs = False\n",
    "        for index_2,sequence in enumerate(fragments):\n",
    "            for index_3,transit in enumerate(sequence):\n",
    "                if not str(transit)[0] == str(transit)[1]: # remove repeat pokes\n",
    "                    if str(transit)[0] == '2':\n",
    "                        seqs = True\n",
    "                        current_newseqs = current_newseqs + [[]]\n",
    "                        current_newlatencies = current_newlatencies + [[]]\n",
    "                        count = count + 1\n",
    "                        current_newseqs[count] = current_newseqs[count] + [transit]\n",
    "                        current_newlatencies[count] = current_newlatencies[count] + [timesplitlatencies[index_1][index_2][index_3]]\n",
    "                    elif seqs == True:\n",
    "                        current_newseqs[count] = current_newseqs[count] + [transit]   \n",
    "                        current_newlatencies[count] = current_newlatencies[count] + [timesplitlatencies[index_1][index_2][index_3]]\n",
    "            seqs = False\n",
    " \n",
    "        newseqs = newseqs + [current_newseqs]\n",
    "        newlatencies = newlatencies + [current_newlatencies]\n",
    "    return(newseqs,newlatencies)\n",
    "\n",
    "def generate_processed_transitiontimesdataframe(processed_seqs,processed_latencies,counter):\n",
    "\n",
    "    count = counter\n",
    "    transits= []\n",
    "    trial_number= []\n",
    "    for fragment in processed_seqs:\n",
    "        count = count + 1\n",
    "        if len(fragment) > 0:\n",
    "            for sequence in fragment:\n",
    "                for transit in sequence:\n",
    "                    trial_number = trial_number + [count]\n",
    "                    transits = transits + [transit]\n",
    "        else: ### deals with cases where there are no good transitions in a trial \n",
    "            transits = transits + ['nan']\n",
    "            trial_number = trial_number + [count]\n",
    "\n",
    "    times = []\n",
    "    for fragment in processed_latencies:\n",
    "        if len(fragment) > 0:\n",
    "            for sequence in fragment:\n",
    "                for time in sequence:\n",
    "                    times = times + [time]\n",
    "        else:\n",
    "            times = times + ['nan']\n",
    "\n",
    "    Processesed_Transition_Latencies = pd.DataFrame({'Trial': trial_number, 'Transitions' : transits,'Latencies' : times})\n",
    "\n",
    "    return(Processesed_Transition_Latencies,count)\n",
    "\n",
    "def sequence_contains_sequence(haystack_seq, needle_seq):\n",
    "    for i in range(0, len(haystack_seq) - len(needle_seq) + 1):\n",
    "        if needle_seq == haystack_seq[i:i+len(needle_seq)]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def convolve_movmean(y,N):\n",
    "    y_padded = np.pad(y, (N//2, N-1-N//2), mode='edge')\n",
    "    y_smooth = np.convolve(y_padded, np.ones((N,))/N, mode='valid') \n",
    "    return y_smooth\n",
    "\n",
    "def SaveFig(file_name,figure_dir):\n",
    "    if not os.path.isdir(figure_dir):\n",
    "        os.makedirs(figure_dir)\n",
    "    plt.savefig(figure_dir + file_name, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paths\n",
    "\n",
    "\n",
    "animal = 'seq006_implant1'\n",
    "\n",
    "path = r'Z:\\projects\\sequence_squad\\revision_data\\organised_data\\animals\\\\' + animal + '\\\\'\n",
    "\n",
    "# just to create baseline files that will be edited later:\n",
    "create_intervales_txt = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording10_27-11-2024\n",
      "recording11_28-11-2024\n",
      "recording1_15-11-2024\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Z:\\\\projects\\\\sequence_squad\\\\revision_data\\\\organised_data\\\\animals\\\\\\\\seq006_implant1\\\\recording1_15-11-2024\\\\behav_sync\\\\2_task\\\\Transition_data_sync.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m      8\u001b[0m         current_path \u001b[38;5;241m=\u001b[39m  os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_path,file) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m          \n\u001b[1;32m----> 9\u001b[0m sync_data \u001b[38;5;241m=\u001b[39m  \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransition_data_sync.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m sync_data2 \u001b[38;5;241m=\u001b[39m  pd\u001b[38;5;241m.\u001b[39mread_csv(current_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBehav_Ephys_Camera_Sync.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path,recording,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost_process_ppseq\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m \n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\miniconda\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Z:\\\\projects\\\\sequence_squad\\\\revision_data\\\\organised_data\\\\animals\\\\\\\\seq006_implant1\\\\recording1_15-11-2024\\\\behav_sync\\\\2_task\\\\Transition_data_sync.csv'"
     ]
    }
   ],
   "source": [
    "for recording in os.listdir(path):\n",
    "    if not 'Store' in recording: # ignore ds store thing\n",
    "        if 'recording' in recording:\n",
    "            print(recording)\n",
    "            current_path = os.path.join(path,recording,'behav_sync') + '\\\\'\n",
    "            for file in os.listdir(current_path):\n",
    "                if 'task' in file:\n",
    "                    current_path =  os.path.join(current_path,file) + '\\\\' \n",
    "            try:         \n",
    "                sync_data =  pd.read_csv(current_path + 'Transition_data_sync.csv')\n",
    "                sync_data2 =  pd.read_csv(current_path + 'Behav_Ephys_Camera_Sync.csv')\n",
    "\n",
    "                output_path = os.path.join(path,recording,'post_process_ppseq') + '\\\\' \n",
    "\n",
    "                # these are important for concainating trials later on!\n",
    "                counter1 = -1\n",
    "                counter2 = -1\n",
    "\n",
    "                #split data by trials \n",
    "                trial_split_data = dict(tuple(sync_data.groupby('Trial_id')))\n",
    "\n",
    "                # pull out transitions and timefilter data for each trial:\n",
    "                transitions = []\n",
    "                Tfilters= [[],[]]\n",
    "                latencies = [[],[]]\n",
    "                for i in trial_split_data:\n",
    "                    transitions = transitions + [list(trial_split_data[i].loc[:,'Transition_type'])]\n",
    "                    Tfilters[0] = Tfilters[0] + [list(trial_split_data[i].loc[:,'2s_Time_Filter_out_in'])]\n",
    "                    latencies[0] = latencies[0] +[list(trial_split_data[i].loc[:,'out_in_Latency'])]   \n",
    "                    # in in\n",
    "                    Tfilters[1] = Tfilters[1] + [list(trial_split_data[i].loc[:,'2s_Time_Filter_in_in'])]\n",
    "                    latencies[1] = latencies[1] +[list(trial_split_data[i].loc[:,'in_in_Latency'])]    \n",
    "\n",
    "                # for each trial,remove transntions that were too long and split into reaminign time relevant fragments - but for both latency types, hence the loop\n",
    "                timesplitseqs = [[],[]]\n",
    "                for i in range(2):\n",
    "                    Tfilt = Tfilters[i] # use out to in pokes first then in in .\n",
    "                    for trial_index,time_filter in enumerate(Tfilt):\n",
    "                        start_end_inds = list(np.where(np.array(time_filter)[:-1] != np.array(time_filter)[1:])[0])\n",
    "                        split = parts(transitions[trial_index],list(np.array(start_end_inds)+1))\n",
    "                        split2 = parts(Tfilt[trial_index],list(np.array(start_end_inds)+1))\n",
    "                        TfiltSplit = RemoveSlowSequences(split,split2)\n",
    "                        del split[::2] # remove every 2nd item eg. all the transitions that were timefilter = 0 so were too long. \n",
    "                        timesplitseqs[i] = timesplitseqs[i] + [TfiltSplit]\n",
    "\n",
    "                ## do the exact same for latency - but for both latency types, hence the loop:\n",
    "                timesplitlatencies = [[],[]]\n",
    "                for i in range(2):\n",
    "                    Tfilt = Tfilters[i] \n",
    "                    latency = latencies[i]\n",
    "                    for trial_index,time_filter in enumerate(Tfilt):\n",
    "                        start_end_inds = list(np.where(np.array(time_filter)[:-1] != np.array(time_filter)[1:])[0])\n",
    "                        split = parts(latency[trial_index],list(np.array(start_end_inds)+1))\n",
    "                        split2 = parts(Tfilt[trial_index],list(np.array(start_end_inds)+1))\n",
    "                        TfiltSplit = RemoveSlowSequences(split,split2)\n",
    "                        del split[::2] # remove every 2nd item eg. all the latencies that were timefilter = 0 so were too long. \n",
    "                        timesplitlatencies[i] = timesplitlatencies[i] + [TfiltSplit]\n",
    "\n",
    "                # for fragments in each trial,sort and trim so that seqs start at initiation port poke and then remove fragments that are too short. ie. remove any transitions sequences that dont inlcude the first port or are just a single transition.\n",
    "                processed_seqs,processed_latencies = aligntofirstpokeandremovesingletransits(timesplitseqs[0],timesplitlatencies[0])  ## use  timesplitlatencies[0] for Out to in Transition times \n",
    "\n",
    "                ## generate processed transition times dataframe:\n",
    "                Processesed_Transition_Latencies_df,counter1 = generate_processed_transitiontimesdataframe(processed_seqs,processed_latencies,counter1)\n",
    "\n",
    "                ## determine perfect sequences and correspondng training level and shaping parameters\n",
    "                trial_perfects = []\n",
    "                T_CorrectScores = [[],[],[],[]]\n",
    "                T_RepeatScores = [[],[],[],[]]\n",
    "\n",
    "                for trial_index,fragments in enumerate(processed_seqs):\n",
    "                    perfect = []\n",
    "                    for fragment in fragments:\n",
    "                        if sequence_contains_sequence(fragment,[21, 16, 63, 37]):\n",
    "                            perfect = perfect + [1]\n",
    "                        else:\n",
    "                            perfect = perfect + [0]\n",
    "\n",
    "                    trial_perfects = trial_perfects + [perfect]  \n",
    "\n",
    "                # calculate mean for each trial:\n",
    "                perfectscore_trials = []\n",
    "                for trial in trial_perfects:\n",
    "                    if len(trial) == 0:\n",
    "                        perfectscore_trials = perfectscore_trials + [0]\n",
    "                    else:\n",
    "                        perfectscore_trials = perfectscore_trials + [np.mean(trial)]\n",
    "\n",
    "                first_p_ephys_time = sync_data2.FirstPoke_EphysTime.values\n",
    "                first_p_ephys_time = first_p_ephys_time[~np.isnan(first_p_ephys_time)]\n",
    "\n",
    "        #         ### just for EJT148 because of missing p1in times:\n",
    "        #         first_p_ephys_time =[]\n",
    "        #         for i in trial_split_data:\n",
    "        #             pokein_times = trial_split_data[i].P1_IN_Ephys_TS.values\n",
    "        #             port2in = np.where(trial_split_data[i].Start_Port==2)[0][0]\n",
    "        #             first_p_ephys_time = first_p_ephys_time + [pokein_times[port2in]]\n",
    "\n",
    "                fig, ax = plt.subplots(1, 1,figsize=(20,10))\n",
    "                ax.plot(first_p_ephys_time,convolve_movmean(perfectscore_trials,20))\n",
    "                ax.set_xlabel('trials',fontsize = 15)\n",
    "                ax.set_ylabel('performance score',fontsize = 15)\n",
    "                SaveFig('Performance score.png',output_path)\n",
    "\n",
    "                out_df = pd.DataFrame({'ephys_time' : first_p_ephys_time ,\n",
    "                                'Convolved_perfromance_score' : convolve_movmean(perfectscore_trials,20)})\n",
    "\n",
    "                out_df.to_csv(output_path + '/Performance_score.csv')\n",
    "                \n",
    "                if create_intervales_txt:\n",
    "                    ## create a txt file with example intervals for the perfect sequences:\n",
    "                    text = 'Pre_sleep = [[0,100]]\\nPost_sleep = [[200,300]]\\nAwake = [[100,200]]'\n",
    "                    with open(output_path + 'Time_intervales.txt', 'w') as f:\n",
    "                        f.write(text)\n",
    "                \n",
    "            except:\n",
    "                print('no data for this recording')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
